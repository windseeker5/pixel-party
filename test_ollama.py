#!/usr/bin/env python3
"""
Comprehensive test script for Ollama AI suggestions debugging.
This will help us identify exactly why AI suggestions aren't working.
"""

import asyncio
import aiohttp
import json
import sys
import time
import traceback

def test_ollama_basic_connection():
    """Test basic Ollama server connection."""
    print("=" * 60)
    print("TEST 1: Basic Ollama Connection")
    print("=" * 60)

    import requests
    try:
        response = requests.get("http://127.0.0.1:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            print(f"‚úÖ Ollama server is running")
            print(f"üì¶ Available models: {models}")
            return True, models
        else:
            print(f"‚ùå Ollama server returned status code: {response.status_code}")
            return False, []
    except Exception as e:
        print(f"‚ùå Failed to connect to Ollama: {e}")
        return False, []

def test_mood_detection():
    """Test mood detection function."""
    print("\n" + "=" * 60)
    print("TEST 2: Mood Detection")
    print("=" * 60)

    # Import the mood detection function
    sys.path.append('/home/kdresdell/Documents/DEV/PixelParty')
    from utils.ollama_client import OllamaClient

    ollama = OllamaClient()

    test_queries = [
        "happy",
        "romantic songs",
        "upbeat music",
        "Beatles",
        "Let It Be",
        "party vibes",
        "sad music"
    ]

    for query in test_queries:
        is_mood = ollama.is_mood_query(query)
        status = "‚úÖ MOOD" if is_mood else "‚ùå NOT MOOD"
        print(f"{status}: '{query}' -> {is_mood}")

    return True

async def test_ollama_direct_api():
    """Test direct Ollama API call for song suggestions."""
    print("\n" + "=" * 60)
    print("TEST 3: Direct Ollama API Call")
    print("=" * 60)

    prompt = """Suggest 5 songs that match this mood: "happy"

Return ONLY a JSON array with this exact format:
[
  {"title": "Song Title", "artist": "Artist Name", "album": "Album Name"},
  {"title": "Song Title", "artist": "Artist Name", "album": "Album Name"},
  {"title": "Song Title", "artist": "Artist Name", "album": "Album Name"},
  {"title": "Song Title", "artist": "Artist Name", "album": "Album Name"},
  {"title": "Song Title", "artist": "Artist Name", "album": "Album Name"}
]

No other text, just the JSON array."""

    payload = {
        "model": "llama3.2:1b",
        "prompt": prompt,
        "stream": False,
        "temperature": 0.7
    }

    try:
        timeout = aiohttp.ClientTimeout(total=30)  # Longer timeout for testing
        async with aiohttp.ClientSession(timeout=timeout) as session:
            print(f"üîÑ Sending request to Ollama...")
            print(f"üìù Prompt length: {len(prompt)} chars")

            start_time = time.time()
            async with session.post("http://127.0.0.1:11434/api/generate", json=payload) as response:
                elapsed = time.time() - start_time
                print(f"‚è±Ô∏è  Response received in {elapsed:.2f} seconds")
                print(f"üìä HTTP Status: {response.status}")

                if response.status == 200:
                    data = await response.json()
                    response_text = data.get('response', '').strip()

                    print(f"üìù Raw response length: {len(response_text)} chars")
                    print(f"üìÑ First 200 chars: {response_text[:200]}...")
                    print(f"üìÑ Last 200 chars: ...{response_text[-200:]}")

                    # Try to parse JSON
                    try:
                        suggestions = json.loads(response_text)
                        print(f"‚úÖ JSON parsing successful!")
                        print(f"üìã Suggestions count: {len(suggestions)}")
                        for i, song in enumerate(suggestions):
                            print(f"  {i+1}. {song.get('title', 'N/A')} - {song.get('artist', 'N/A')}")
                        return True, suggestions
                    except json.JSONDecodeError as e:
                        print(f"‚ùå JSON parsing failed: {e}")
                        print(f"üîß Trying to extract JSON from markdown...")

                        # Try markdown extraction
                        import re
                        json_pattern = r'```(?:json)?\s*(\[.*?\])\s*```'
                        match = re.search(json_pattern, response_text, re.DOTALL)
                        if match:
                            json_str = match.group(1)
                            try:
                                suggestions = json.loads(json_str)
                                print(f"‚úÖ Markdown JSON extraction successful!")
                                print(f"üìã Suggestions count: {len(suggestions)}")
                                return True, suggestions
                            except json.JSONDecodeError:
                                print(f"‚ùå Markdown JSON extraction also failed")

                        print(f"üîç Raw response for debugging:")
                        print(f"'{response_text}'")
                        return False, []
                else:
                    error_text = await response.text()
                    print(f"‚ùå HTTP Error {response.status}: {error_text}")
                    return False, []

    except asyncio.TimeoutError:
        print(f"‚ùå Request timed out after 30 seconds")
        return False, []
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        print(f"üîç Traceback: {traceback.format_exc()}")
        return False, []

def test_ollama_client_sync():
    """Test the OllamaClient sync wrapper."""
    print("\n" + "=" * 60)
    print("TEST 4: OllamaClient Sync Wrapper")
    print("=" * 60)

    sys.path.append('/home/kdresdell/Documents/DEV/PixelParty')
    from utils.ollama_client import OllamaClient

    ollama = OllamaClient()

    try:
        print(f"üîÑ Testing mood detection...")
        is_mood = ollama.is_mood_query("happy")
        print(f"‚úÖ Mood detection: 'happy' -> {is_mood}")

        if is_mood:
            print(f"üîÑ Getting song suggestions...")
            start_time = time.time()
            suggestions = ollama.get_song_suggestions("happy")
            elapsed = time.time() - start_time

            print(f"‚è±Ô∏è  Sync call completed in {elapsed:.2f} seconds")
            print(f"üìã Suggestions count: {len(suggestions) if suggestions else 0}")

            if suggestions:
                print(f"‚úÖ Sync wrapper working!")
                for i, song in enumerate(suggestions):
                    print(f"  {i+1}. {song.get('title', 'N/A')} - {song.get('artist', 'N/A')}")
                return True, suggestions
            else:
                print(f"‚ùå No suggestions returned")
                return False, []
        else:
            print(f"‚ùå Mood detection failed")
            return False, []

    except Exception as e:
        print(f"‚ùå OllamaClient error: {e}")
        print(f"üîç Traceback: {traceback.format_exc()}")
        return False, []

def test_flask_route_simulation():
    """Test the Flask route logic simulation."""
    print("\n" + "=" * 60)
    print("TEST 5: Flask Route Logic Simulation")
    print("=" * 60)

    # Simulate the Flask route logic
    search_query = "happy"

    # Mock Flask app logger
    class MockLogger:
        def info(self, msg): print(f"INFO: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
        def warning(self, msg): print(f"WARNING: {msg}")

    class MockCurrentApp:
        logger = MockLogger()

    # Mock the current_app
    import sys
    sys.path.append('/home/kdresdell/Documents/DEV/PixelParty')

    try:
        from utils.ollama_client import OllamaClient
        ollama = OllamaClient()

        current_app = MockCurrentApp()  # Mock current_app

        current_app.logger.info(f"Getting AI suggestions for: '{search_query}'")

        # First check if this is actually a mood query
        is_mood = ollama.is_mood_query(search_query)
        current_app.logger.info(f"Is mood query: {is_mood}")

        if not is_mood:
            current_app.logger.info(f"'{search_query}' not detected as mood query, skipping AI suggestions")
            return False, "Not a mood query"

        ai_suggestions = ollama.get_song_suggestions(search_query)
        current_app.logger.info(f"Got {len(ai_suggestions) if ai_suggestions else 0} AI suggestions")

        if ai_suggestions:
            print(f"‚úÖ Flask route simulation successful!")
            return True, ai_suggestions
        else:
            print(f"‚ùå No AI suggestions generated")
            return False, "No suggestions"

    except Exception as e:
        print(f"‚ùå Flask simulation error: {e}")
        print(f"üîç Traceback: {traceback.format_exc()}")
        return False, str(e)

async def main():
    """Run all tests."""
    print("üß™ OLLAMA AI SUGGESTIONS COMPREHENSIVE TEST SUITE")
    print("üéØ Goal: Debug why AI suggestions show 'temporarily unavailable'")
    print("")

    results = {}

    # Test 1: Basic connection
    results['connection'], models = test_ollama_basic_connection()

    # Test 2: Mood detection
    results['mood_detection'] = test_mood_detection()

    # Test 3: Direct API call
    results['direct_api'], direct_suggestions = await test_ollama_direct_api()

    # Test 4: Sync wrapper
    results['sync_wrapper'], sync_suggestions = test_ollama_client_sync()

    # Test 5: Flask route simulation
    results['flask_simulation'], flask_result = test_flask_route_simulation()

    # Summary
    print("\n" + "=" * 60)
    print("üéØ TEST RESULTS SUMMARY")
    print("=" * 60)

    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status}: {test_name}")

    total_passed = sum(results.values())
    total_tests = len(results)

    print(f"\nüìä Overall: {total_passed}/{total_tests} tests passed")

    if total_passed == total_tests:
        print("üéâ All tests passed! AI suggestions should be working.")
    else:
        print("üîç Some tests failed. Check the output above for debugging info.")

        # Specific debugging advice
        if not results['connection']:
            print("üí° Fix: Ensure Ollama server is running on port 11434")
        elif not results['direct_api']:
            print("üí° Fix: Check Ollama model response format or increase timeout")
        elif not results['sync_wrapper']:
            print("üí° Fix: Debug the async/sync conversion in OllamaClient")
        elif not results['flask_simulation']:
            print("üí° Fix: Check Flask integration and error handling")

if __name__ == "__main__":
    asyncio.run(main())